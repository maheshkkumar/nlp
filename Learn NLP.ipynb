{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Wikipedia says 'Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages and, in particular, concerned with programming computers to fruitfully process large natural language corpora'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP required?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several reasons that made NLP popular within the Artificial Intelligence and Machine Learning community, especially the huge opportunity that NLP provided for everyone to work on large sets of textual data.\n",
    "\n",
    "Few of the domains, where NLP plays a crucial role is:\n",
    "\n",
    "* Auto Summarization\n",
    "* Chatbots\n",
    "* Machine Translation\n",
    "* Text Classification\n",
    "* Sentimental Analysis\n",
    "* Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps taken to solve a NLP problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, let's try to classify news's articles to different categories based on the article content and in doing so let's look at the various steps that we take to solve a NLP problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Collection of the Dataset\n",
    "\n",
    "The first and foremost step in any Machine Learning or NLP based problem is to collect a dataset. For our problem, let's try to collect news articles from [Doxydonkey](http://www.doxydonkey.blogspot.in).\n",
    "\n",
    "Let's scrape all the articles from the above mentioned website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# URL to collect the data\n",
    "url = \"http://doxydonkey.blogspot.in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to request, response and soup the data\n",
    "def beautifySoup(url):\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting all articles hyer links\n",
    "def collectArticles(links):\n",
    "    articles = []\n",
    "    for link in links:\n",
    "        content = []\n",
    "        soup = beautifySoup(link)\n",
    "        divs = soup.find_all('div', {'class': 'post-body'})\n",
    "        for div in divs:\n",
    "            content += map(lambda p: p.text.encode(\"ascii\", errors=\"replace\").replace(\"?\", \"\"), div.find_all(\"li\"))\n",
    "        articles += content\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collectData(url, links):\n",
    "    soup = beautifySoup(url)\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            title = a[\"title\"]\n",
    "            url = a[\"href\"]\n",
    "            if title == \"Older Posts\":\n",
    "                links.append(url)\n",
    "                getAllLinks(url, links)\n",
    "        except:\n",
    "            url = \"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "def collectParameters():\n",
    "    links.append(url)\n",
    "    collectData(url, links)\n",
    "    print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2017-02-20T18:07:00-08:00&max-results=7&start=7&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2017-02-09T18:22:00-08:00&max-results=7&start=14&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2017-01-31T17:57:00-08:00&max-results=7&start=21&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2017-01-19T17:44:00-08:00&max-results=7&start=28&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2017-01-10T18:06:00-08:00&max-results=7&start=35&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2017-01-01T18:20:00-08:00&max-results=7&start=42&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-12-21T18:55:00-08:00&max-results=7&start=49&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-12-12T19:04:00-08:00&max-results=7&start=56&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-09-26T00:28:00-07:00&max-results=7&start=63&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-09-08T03:41:00-07:00&max-results=7&start=70&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-08-25T21:48:00-07:00&max-results=7&start=77&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-08-16T19:49:00-07:00&max-results=7&start=84&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-08-04T20:26:00-07:00&max-results=7&start=91&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-07-25T19:21:00-07:00&max-results=7&start=98&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-07-14T19:28:00-07:00&max-results=7&start=105&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-07-05T19:56:00-07:00&max-results=7&start=112&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-06-23T19:33:00-07:00&max-results=7&start=119&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-06-14T19:51:00-07:00&max-results=7&start=126&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-06-05T19:10:00-07:00&max-results=7&start=133&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-05-25T19:22:00-07:00&max-results=7&start=140&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-05-15T19:31:00-07:00&max-results=7&start=147&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-05-05T19:42:00-07:00&max-results=7&start=154&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-04-26T19:14:00-07:00&max-results=7&start=161&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-04-18T19:30:00-07:00&max-results=7&start=168&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-04-07T19:24:00-07:00&max-results=7&start=175&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-03-29T18:51:00-07:00&max-results=7&start=182&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-03-17T19:23:00-07:00&max-results=7&start=189&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-03-08T18:01:00-08:00&max-results=7&start=196&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-02-25T18:05:00-08:00&max-results=7&start=203&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-02-16T18:23:00-08:00&max-results=7&start=210&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-02-07T18:13:00-08:00&max-results=7&start=217&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-01-27T18:51:00-08:00&max-results=7&start=224&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-01-17T18:38:00-08:00&max-results=7&start=231&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2016-01-06T18:46:00-08:00&max-results=7&start=238&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-12-27T18:02:00-08:00&max-results=7&start=245&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-12-15T21:48:00-08:00&max-results=7&start=252&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-12-06T18:18:00-08:00&max-results=7&start=259&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-11-25T18:16:00-08:00&max-results=7&start=266&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-11-16T18:14:00-08:00&max-results=7&start=273&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-11-04T21:09:00-08:00&max-results=7&start=280&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-10-26T19:45:00-07:00&max-results=7&start=287&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-10-11T19:27:00-07:00&max-results=7&start=294&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-09-29T19:33:00-07:00&max-results=7&start=301&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-09-17T19:39:00-07:00&max-results=7&start=308&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-09-08T19:50:00-07:00&max-results=7&start=315&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-08-30T19:18:00-07:00&max-results=7&start=322&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-08-19T19:32:00-07:00&max-results=7&start=329&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-08-10T19:21:00-07:00&max-results=7&start=336&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-07-30T18:48:00-07:00&max-results=7&start=343&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-07-21T19:19:00-07:00&max-results=7&start=350&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-07-12T19:24:00-07:00&max-results=7&start=357&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-07-01T19:15:00-07:00&max-results=7&start=364&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-06-22T19:21:00-07:00&max-results=7&start=371&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-06-11T19:05:00-07:00&max-results=7&start=378&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-06-02T19:38:00-07:00&max-results=7&start=384&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-05-24T20:16:00-07:00&max-results=7&start=391&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-05-13T20:18:00-07:00&max-results=7&start=398&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-05-04T20:23:00-07:00&max-results=7&start=405&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-04-23T20:19:00-07:00&max-results=7&start=412&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-04-14T19:40:00-07:00&max-results=7&start=419&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-04-05T20:22:00-07:00&max-results=7&start=426&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-03-24T20:12:00-07:00&max-results=7&start=433&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-03-15T20:41:00-07:00&max-results=7&start=440&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-03-03T19:30:00-08:00&max-results=7&start=447&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-02-22T19:55:00-08:00&max-results=7&start=454&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-02-11T20:02:00-08:00&max-results=7&start=461&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-02-02T19:46:00-08:00&max-results=7&start=468&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-01-22T19:50:00-08:00&max-results=7&start=474&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-01-15T19:17:00-08:00&max-results=7&start=479&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2015-01-06T19:48:00-08:00&max-results=7&start=486&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-12-25T21:30:00-08:00&max-results=7&start=493&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-12-15T19:24:00-08:00&max-results=7&start=500&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-12-05T01:52:00-08:00&max-results=7&start=507&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-11-26T01:44:00-08:00&max-results=7&start=514&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-11-17T01:41:00-08:00&max-results=7&start=521&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-11-06T01:38:00-08:00&max-results=7&start=528&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-10-28T01:24:00-07:00&max-results=7&start=535&by-date=false')\n",
      "('Older Posts', 'http://doxydonkey.blogspot.in/search?updated-max=2014-10-17T01:20:00-07:00&max-results=7&start=542&by-date=false')\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "collectParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Airbnb raises $1 billion inlatestround of funding:Onlineroom rentingservice Airbnb Inc said on Thursday it had raised $1 billion in its latest round of funding, valuing the company at $31 billion.The company turned in a profit on an EBITDA basis in the second half of 2016 and expects to continue to be profitable this year, the source said, adding that Airbnb had no plans to go public anytime soon.The company is locked in an intensifying global battle with regulators who say the service takesaffordablehousing off the market and drives up rental prices.Airbnb raised $447.85 million as part of the funding, a source close to the company told Reuters. The company said in September it had raised about $555 million as part of the same round of funding.Airbnb, which operates in more than 65,000 cities, has enjoyed tremendous growth as it pushes ahead with its plansofglobal expansion.'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = collectArticles(links)\n",
    "print(len(articles))\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Term Frequency - Inverse Document Frequency\n",
    "\n",
    "TF-IDF weight is often used in information retrieval and text mining. The weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.\n",
    "\n",
    "TF-IDF is made up of two components: \n",
    "* TF - Term Frequency\n",
    "        \n",
    "        Term Frequency is the measure of how frequently a word appears in a document.\n",
    "        TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "        \n",
    "* IDF - Inverse Document Frequency\n",
    "        Inverse Document Frequency the measure of how important a term is.\n",
    "        IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing feature extractor TF-IDF vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4527)\t0.093459986294\n",
      "  (0, 880)\t0.0909888475678\n",
      "  (0, 9245)\t0.125506358268\n",
      "  (0, 5484)\t0.0593929321553\n",
      "  (0, 12111)\t0.136331194112\n",
      "  (0, 4293)\t0.136331194112\n",
      "  (0, 2522)\t0.0853580208378\n",
      "  (0, 1)\t0.065168725165\n",
      "  (0, 470)\t0.109464227828\n",
      "  (0, 8136)\t0.103883585377\n",
      "  (0, 427)\t0.158782106766\n",
      "  (0, 10394)\t0.0896669240819\n",
      "  (0, 9883)\t0.0904124688247\n",
      "  (0, 11941)\t0.0768705699236\n",
      "  (0, 2603)\t0.0766677306981\n",
      "  (0, 7515)\t0.0847935427048\n",
      "  (0, 539)\t0.113991816532\n",
      "  (0, 363)\t0.169920689887\n",
      "  (0, 8980)\t0.0869161877993\n",
      "  (0, 9696)\t0.121217078597\n",
      "  (0, 4015)\t0.123688217323\n",
      "  (0, 7265)\t0.0517256017034\n",
      "  (0, 10412)\t0.0532921082588\n",
      "  (0, 10199)\t0.0728780620055\n",
      "  (0, 9604)\t0.100322272811\n",
      "  :\t:\n",
      "  (0, 9194)\t0.0657727820619\n",
      "  (0, 8711)\t0.0676923544855\n",
      "  (0, 735)\t0.0880771876065\n",
      "  (0, 10875)\t0.177538939725\n",
      "  (0, 13072)\t0.0378691826488\n",
      "  (0, 9066)\t0.0955164362715\n",
      "  (0, 2980)\t0.0875720285004\n",
      "  (0, 4534)\t0.102520325273\n",
      "  (0, 180)\t0.0853580208378\n",
      "  (0, 5550)\t0.0794643241154\n",
      "  (0, 10304)\t0.0705467582402\n",
      "  (0, 1598)\t0.100928007859\n",
      "  (0, 4101)\t0.133419534151\n",
      "  (0, 9064)\t0.0717214619913\n",
      "  (0, 12206)\t0.0983256447076\n",
      "  (0, 277)\t0.0884205814672\n",
      "  (0, 12531)\t0.120451910281\n",
      "  (0, 10049)\t0.14874067677\n",
      "  (0, 6779)\t0.0767689144505\n",
      "  (0, 9353)\t0.213261264741\n",
      "  (0, 11862)\t0.0730484148993\n",
      "  (0, 5171)\t0.298211234269\n",
      "  (0, 1741)\t0.127885890416\n",
      "  (0, 9354)\t0.0950423945693\n",
      "  (0, 892)\t0.545073115219\n"
     ]
    }
   ],
   "source": [
    "# Applying tf-idf transformation on the articles \n",
    "\n",
    "articles_tfidf = vectorizer.fit_transform(articles)\n",
    "\n",
    "# Let's see the tf-idf for the first article\n",
    "print(articles_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Clustering the articles\n",
    "\n",
    "Next, let's cluster the above articles in order to predict the category of future articles.\n",
    "\n",
    "Let's use KMeans algorithm to cluster the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing KMeans algorithm from sklearn\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instance of KMeans\n",
    "\n",
    "km = KMeans(n_clusters=10, init='k-means++', n_init=2, max_iter=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 4819.099\n",
      "Iteration  1, inertia 2499.371\n",
      "Iteration  2, inertia 2485.310\n",
      "Iteration  3, inertia 2478.664\n",
      "Iteration  4, inertia 2474.163\n",
      "Iteration  5, inertia 2470.683\n",
      "Iteration  6, inertia 2468.460\n",
      "Iteration  7, inertia 2467.534\n",
      "Iteration  8, inertia 2467.211\n",
      "Iteration  9, inertia 2467.074\n",
      "Iteration 10, inertia 2467.046\n",
      "Iteration 11, inertia 2467.038\n",
      "Iteration 12, inertia 2467.030\n",
      "Iteration 13, inertia 2467.024\n",
      "Converged at iteration 13\n",
      "Initialization complete\n",
      "Iteration  0, inertia 2648.611\n",
      "Iteration  1, inertia 2563.164\n",
      "Iteration  2, inertia 2541.464\n",
      "Iteration  3, inertia 2533.711\n",
      "Iteration  4, inertia 2532.416\n",
      "Iteration  5, inertia 2528.497\n",
      "Iteration  6, inertia 2525.316\n",
      "Iteration  7, inertia 2524.910\n",
      "Iteration  8, inertia 2524.788\n",
      "Iteration  9, inertia 2524.737\n",
      "Iteration 10, inertia 2524.723\n",
      "Converged at iteration 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=10, n_init=2,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=True)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model with the articles dataset\n",
    "\n",
    "km.fit(articles_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have fitted the algorithm with the dataset. Now, let's check the type of articles that are clustered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of each article: [0 5 2 ..., 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "# Label that is assigned to each articles \n",
    "\n",
    "print(\"Label of each article: {}\".format(km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in each cluster: (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 224,  269,  155,  109,  253,  179,  147,  168, 1002,  167], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Total number of articles in each cluster\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cluster_count = np.unique(km.labels_, return_counts=True)\n",
    "print(\"Number of articles in each cluster: {}\".format(cluster_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's segregate the articles into their respective clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Segregating the documents according to the clusters\n",
    "\n",
    "article_clusters = {}\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    document = articles[i]\n",
    "    if cluster not in article_clusters.keys():\n",
    "        article_clusters[cluster] = document\n",
    "    else:\n",
    "        article_clusters[cluster] += document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tokenizing the collected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step in the process is to tokenizing the collected data. What do you mean by vectorizing the data?\n",
    "\n",
    "Tokenization is the process of breaking down the complex sentences to individual words.\n",
    "\n",
    "> \"I like dogs\"\n",
    "\n",
    "So, the above statement can be tokenized to get (\"I\", \"like\", \"dogs\"), this is similar to using **split()** method to tokenize a sentence.\n",
    "\n",
    "In terms of NLP, tokens better known as **n-grams**, where one word is called **unigram**, 2 words are called as **bigram** and multiple words are called a **n-grams**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words are: [u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'billion', 'million', 'billions', 'millions', '$', \"'s\", \"'d\", '\\n', '\\t', \"''\", '``', '`', '*']\n"
     ]
    }
   ],
   "source": [
    "# Let's declare a list of some not so important words that might appear in the articles\n",
    "\n",
    "special_words = [\"billion\", \"million\", \"billions\", \"millions\", \"$\",  \"'s\", \"'d\", \"\\n\", \"\\t\", \"''\", \"``\", \"`\", \"*\"]\n",
    "stop_words = list(set(stopwords.words(\"english\"))) + list(punctuation) + special_words\n",
    "\n",
    "print(\"Stop words are: {}\".format(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will see the top words from each cluster, this should give us an idea of theme of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ['company', 'uber', 'investors', 'said', 'companies', 'valuation', 'year', 'private', 'round', 'public', 'last', 'according', 'percent', 'new', 'raised', 'market', 'funding', 'lyft', 'people', 'also'])\n",
      "(1, ['percent', 'revenue', 'company', 'quarter', 'year', 'said', 'shares', 'share', 'sales', 'profit', 'rose', 'business', 'cents', 'analysts', 'earnings', 'net', 'growth', 'per', 'fell', 'reported'])\n",
      "(2, ['alibaba', 'percent', 'china', 'company', 'said', 'chinese', 'e-commerce', 'new', 'online', 'market', 'year', 'shares', 'group', 'also', 'would', 'stake', 'chinas', 'companies', 'internet', 'u.s.'])\n",
      "(3, ['twitter', 'company', 'users', 'said', 'new', 'percent', 'social', 'facebook', 'tweets', 'twitters', 'also', 'people', 'stock', 'like', 'user', 'year', 'app', 'data', 'dorsey', 'product'])\n",
      "(4, ['facebook', 'users', 'ads', 'video', 'new', 'said', 'ad', 'app', 'company', 'people', 'google', 'mobile', 'like', 'percent', 'snapchat', 'also', 'youtube', 'service', 'data', 'instagram'])\n",
      "(5, ['amazon', 'said', 'company', 'service', 'new', 'prime', 'online', 'also', 'delivery', 'percent', 'products', 'amazons', 'year', 'services', 'business', 'customers', 'like', 'items', 'one', 'last'])\n",
      "(6, ['uber', 'said', 'company', 'cars', 'car', 'drivers', 'tesla', 'vehicles', 'new', 'google', 'service', 'also', 'model', 'percent', 'driver', 'self-driving', 'would', 'technology', 'first', 'year'])\n",
      "(7, ['apple', 'said', 'iphone', 'company', 'percent', 'pay', 'new', 'sales', 'watch', 'china', 'apples', 'year', 'also', 'app', 'market', 'mobile', 'according', 'service', 'first', 'would'])\n",
      "(8, ['company', 'said', 'google', 'new', 'companies', 'like', 'year', 'percent', 'also', 'one', 'data', 'last', 'people', 'would', 'business', 'online', 'technology', 'mobile', 'users', 'app'])\n",
      "(9, ['india', 'company', 'said', 'rs', 'crore', 'online', 'year', 'also', 'funding', 'snapdeal', 'capital', 'e-commerce', 'firm', 'round', 'new', 'flipkart', 'raised', 'investors', 'last', 'indian'])\n"
     ]
    }
   ],
   "source": [
    "keywords = {}\n",
    "counts = {}\n",
    "\n",
    "for cluster in range(km.n_clusters):\n",
    "    word_set = word_tokenize(article_clusters[cluster].lower())\n",
    "    word_set = [word for word in word_set if word not in stop_words and word != \"\"]\n",
    "    freq_distribution = FreqDist(word_set)\n",
    "    keywords[cluster] = nlargest(100, freq_distribution, key=freq_distribution.get)\n",
    "    counts[cluster] = freq_distribution\n",
    "\n",
    "# printing the top 100 words from each cluster\n",
    "for key, value in keywords.iteritems():\n",
    "    print(key, value[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's find the unique words in each cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words = {}\n",
    "for cluster in range(km.n_clusters):\n",
    "    temp_cluster_1 = list(set(range(km.n_clusters)) - set([cluster]))\n",
    "    temp_cluster_2 = set(keywords[temp_cluster_1[0]]).union(set(keywords[temp_cluster_1[1]]))\n",
    "    unique_word = set(keywords[cluster]) - temp_cluster_2\n",
    "    unique_words[cluster] = nlargest(10, unique_word, key=counts[cluster].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ['uber', 'valuation', 'private', 'round', 'raised', 'lyft', 'funding', 'people', 'capital', 'venture'])\n",
      "(1, ['profit', 'rose', 'cents', 'analysts', 'earnings', 'net', 'per', 'reported', 'trading', 'forecast'])\n",
      "(2, ['alibaba', 'e-commerce', 'group', 'stake', 'chinas', 'internet', 'yahoo', 'alibabas', 'holding', 'tencent'])\n",
      "(3, ['twitter', 'social', 'tweets', 'twitters', 'user', 'dorsey', 'product', 'media', 'ads', 'buy'])\n",
      "(4, ['ads', 'video', 'ad', 'youtube', 'instagram', 'social', 'facebooks', 'advertisers', 'content', 'news'])\n",
      "(5, ['prime', 'delivery', 'amazons', 'products', 'items', 'product', 'amazon.com', 'jet', 'shipping', 'retailer'])\n",
      "(6, ['cars', 'car', 'drivers', 'tesla', 'vehicles', 'model', 'driver', 'self-driving', 'musk', 'india'])\n",
      "(7, ['apple', 'iphone', 'pay', 'watch', 'apples', 'apps', 'iphones', 'store', 'music', 'samsung'])\n",
      "(8, ['internet', 'make', 'use', 'search', 'mr.', 'way', 'products', 'information', 'big', 'made'])\n",
      "(9, ['india', 'rs', 'crore', 'snapdeal', 'e-commerce', 'flipkart', 'indian', 'platform', 'internet', 'us'])\n"
     ]
    }
   ],
   "source": [
    "# printing unique words to a cluster\n",
    "for i, words in enumerate(unique_words):\n",
    "    print(i, unique_words[words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we know what's the theme of each cluster\n",
    "\n",
    "* Cluster 0 - Amazon\n",
    "* Cluster 1 - Ads and Social Media\n",
    "* Cluster 2 - Stock Market\n",
    "* Cluster 3 - Apple\n",
    "* Cluster 4 - Investment\n",
    "\n",
    "Disclaimer - The clusters might not be accurately created, this is done just to give an overview on how the articles will be clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = {0: \"Cab Aggregator\", 1: \"Stocks\", 2: \"Alibaba\", 3: \"Twitter\", 4: \"Ads\", 5: \"Amazon\", 6: \"Self Driving\", 7: \"Apple\", 8: \"Internet\", 9: \"Ecommerce in India\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Predicting the label for an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are using KNeighborsClassifier for predicting the class for an article\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the previously modelled data to KNeighborsClassifier\n",
    "\n",
    "clf.fit(articles_tfidf, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_airbnb = \" Online room renting service Airbnb Inc said on Thursday it had raised $1 billion in its latest round of funding, valuing the company at $31 billion.The company turned in a profit on an EBITDA basis in the second half of 2016 and expects to continue to be profitable this year, the source said, adding that Airbnb had no plans to go public anytime soon. The company is locked in an intensifying global battle with regulators who say the service takes affordable housing off the market and drives up rental prices.Airbnb raised $447.85 million as part of the funding, a source close to the company told Reuters. The company said in September it had raised about $555 million as part of the same round of funding. Airbnb, which operates in more than 65,000 cities, has enjoyed tremendous growth as it pushes ahead with its plans of global expansion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airbnb_transform = vectorizer.transform([predict_airbnb.decode('utf8').encode('ascii', errors='ignore')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category belongs to the category: Internet\n"
     ]
    }
   ],
   "source": [
    "label = clf.predict(airbnb_transform)[0]\n",
    "\n",
    "if label in categories:\n",
    "    print(\"Category belongs to the category: {}\".format(categories[label]))\n",
    "else:\n",
    "    print(\"Unknown category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you understood the basics of a Natural Language Processing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
